# llm-zig-eval Specification

## Overview

A benchmark suite for evaluating LLM performance on Zig programming tasks. The system prompts models via OpenRouter, captures generated code, compiles/tests it, and produces a comparative report with optional multi-model judging.

---

## Architecture

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           USER CLI                                       â”‚
â”‚  $ zig build run -- --models=anthropic/claude-3.5,openai/gpt-4o         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    MAIN ORCHESTRATOR    â”‚â—„â”€â”€â”€â”‚   PROBLEM REGISTRY (problems/*.txt)      â”‚
â”‚    (src/main.zig)       â”‚    â”‚   + Test Harnesses (problems/*_test.zig) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â”‚ HTTP POST /api/v1/chat/completions
            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   OPENROUTER GATEWAY    â”‚
â”‚ (src/gateways/          â”‚
â”‚  openrouter.zig)        â”‚
â”‚  - Bearer Auth          â”‚
â”‚  - Model routing        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚                    OPENROUTER.AI API                        â”‚
   â”‚  Routes to: Anthropic, OpenAI, Meta, DeepSeek, Google, etc. â”‚
   â”‚  Returns: JSON with usage.{prompt_tokens, completion_tokens}â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  RESULT PARSER & WRITER â”‚â”€â”€â”€â–¶â”‚  SANDBOX: ./out/{model}/{problem}.zig    â”‚
â”‚  (src/core/parser.zig)  â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚  - Extract code blocks  â”‚                        â”‚
â”‚  - Capture token usage  â”‚                        â–¼
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚                  â”‚  ZIG COMPILER & TEST RUNNER              â”‚
            â”‚                  â”‚  $ zig test problem_harness.zig          â”‚
            â”‚                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                                      â”‚
            â–¼                                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         REPORT GENERATOR                                 â”‚
â”‚  - Aggregates pass/fail, timing, cost, LOC                              â”‚
â”‚  - Renders ASCII table or JSON                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## File Structure

```
llm-zig-eval/
â”œâ”€â”€ build.zig                 # Main build script
â”œâ”€â”€ build.zig.zon             # Dependencies
â”œâ”€â”€ README.md                 # Project overview
â”œâ”€â”€ SPEC.md                   # This specification
â”œâ”€â”€ .env.example              # Template for API keys
â”‚
â”œâ”€â”€ problems/                 # The "Gauntlet" - benchmark problems
â”‚   â”œâ”€â”€ q1_memory.txt         # Problem 1 prompt
â”‚   â”œâ”€â”€ q1_test.zig           # Problem 1 test harness
â”‚   â”œâ”€â”€ q2_concurrency.txt    # Problem 2 prompt
â”‚   â”œâ”€â”€ q2_test.zig           # Problem 2 test harness
â”‚   â”œâ”€â”€ q3_comptime.txt       # Problem 3 prompt
â”‚   â””â”€â”€ q3_test.zig           # Problem 3 test harness
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.zig              # CLI entry point, orchestration
â”‚   â”œâ”€â”€ config.zig            # .env loader, model cost table
â”‚   â”‚
â”‚   â”œâ”€â”€ gateways/
â”‚   â”‚   â””â”€â”€ openrouter.zig    # HTTP client for OpenRouter API
â”‚   â”‚
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ parser.zig        # Extract code from LLM response
â”‚   â”‚   â”œâ”€â”€ sandbox.zig       # Write files, run zig test
â”‚   â”‚   â”œâ”€â”€ tokens.zig        # Parse usage JSON, calc cost
â”‚   â”‚   â””â”€â”€ reporter.zig      # ASCII table / JSON output
â”‚   â”‚
â”‚   â””â”€â”€ council/
â”‚       â”œâ”€â”€ types.zig         # JudgeVerdict, ConsensusResult
â”‚       â”œâ”€â”€ tribunal.zig      # Multi-judge orchestration
â”‚       â””â”€â”€ prompts.zig       # Judge system prompts
â”‚
â””â”€â”€ docs/                     # Design documentation
    â”œâ”€â”€ gem_init_1.md
    â”œâ”€â”€ gem_init_2.md
    â””â”€â”€ gem_init_3.md
```

---

## Dependencies

### rich_zig

We use [rich_zig](https://github.com/hotschmoe/rich_zig) for terminal output formatting.

**Why?** This tool can run for 60+ seconds. Users shouldn't stare at a silent terminal wondering if it's frozen. We provide:

- **Progress bars** â€” Per-model and per-problem progress tracking
- **Spinners** â€” Visual feedback during API calls and compilation
- **Styled tables** â€” Colored, formatted result tables
- **Status messages** â€” Real-time updates on what's happening

**Example UX Flow:**
```
ğŸ”„ Fetching anthropic/claude-3.5-sonnet...
   â”œâ”€â”€ Problem 1: Arena Allocator â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘ 80%
   â”œâ”€â”€ Problem 2: Mock Socket     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ âœ“
   â””â”€â”€ Problem 3: JSON Parser     â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ pending

â±ï¸  Elapsed: 12.3s | Est. remaining: 8.2s
```

---

## Benchmark Problems (The Gauntlet)

Three problems test core Zig competencies. Each includes a text prompt and a test harness.

### Problem 1: Arena Allocator from Scratch

**Focus:** Memory, pointers, manual layout, alignment

**Prompt Summary:**
> Create a `MiniArena` struct managing a fixed 1024-byte buffer.
> - `alloc(size: usize) ![]u8` â€” returns aligned slice, advances pointer
> - `reset() void` â€” rewinds the pointer
> - Must enforce 8-byte alignment
> - Return `error.OutOfMemory` when full

**Test Harness Validates:**
- Allocations return 8-byte aligned addresses
- Memory can be reused after reset
- OutOfMemory returned correctly

---

### Problem 2: Mock TCP Socket (Async/Threading)

**Focus:** Concurrency, threads, error handling

**Prompt Summary:**
> Create a `MockSocket` struct with:
> - `connect(address: []const u8) !void` â€” simulates network delay with `std.time.sleep`
> - Returns `error.ConnectionRefused` if address is "bad_host"
> - Uses `std.Thread` to run connection non-blocking
> - `isConnected() bool` to check status

**Test Harness Validates:**
- 5 connections run in parallel (total time â‰ˆ 1 sleep, not 5Ã—)
- Error handling for bad_host
- No deadlocks or race conditions

---

### Problem 3: JSON-to-Struct (Comptime Reflection)

**Focus:** Comptime, `@typeInfo`, string parsing

**Prompt Summary:**
> Create `jsonToStruct(comptime T: type, json: []const u8) !T`
> - Uses `@typeInfo` to iterate struct fields at comptime
> - Parses simple JSON without using `std.json`
> - Supports `u8`, `u32`, `[]const u8` field types

**Test Harness Validates:**
- `Point{ .x = 10, .y = 20 }` parsed from `{"x": 10, "y": 20}`
- String fields handled correctly
- Compile-time field iteration (not runtime reflection)

---

## Council of Judges (Multi-Model Consensus)

Optional qualitative evaluation using multiple LLM judges to reduce single-model bias.

### Architecture

```text
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚       CANDIDATE SOLUTION            â”‚
            â”‚   (Generated Zig code)              â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â–¼                  â–¼                  â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ JUDGE A  â”‚       â”‚ JUDGE B  â”‚       â”‚ JUDGE C  â”‚
    â”‚ Claude   â”‚       â”‚ GPT-4o   â”‚       â”‚ DeepSeek â”‚
    â”‚ (Pedant) â”‚       â”‚(Architect)â”‚      â”‚ (Hacker) â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                  â”‚                  â”‚
          â”‚    PHASE 1: BLIND EVALUATION        â”‚
          â–¼                  â–¼                  â–¼
    [Draft Score]      [Draft Score]      [Draft Score]
          â”‚                  â”‚                  â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
              PHASE 2: CROSS-POLLINATION
           "Here's what the others said..."
                             â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â–¼                  â–¼                  â–¼
    [Final Score]      [Final Score]      [Final Score]
          â”‚                  â”‚                  â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â–¼
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚ CONSENSUS ENGINEâ”‚
                   â”‚  Average: 8.5   â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Judge Personas

| Judge | Model | Focus |
|-------|-------|-------|
| A (Pedant) | Claude 3.5 | Safety, `defer`, strict types |
| B (Architect) | GPT-4o | Readability, structure, logic |
| C (Hacker) | DeepSeek/Llama | Performance, cleverness, brevity |

### Grading Rubric

1. **Safety (Critical):** Correct allocator usage, `defer`, no use-after-free
2. **Correctness:** Actually solves the stated problem
3. **Zig-Zen:** Uses optionals (`.?`), `try`, slices over C-style pointers

### Cost Optimization

Council only runs if code **passes** compilation and tests. No point judging broken code.

```zig
if (compile_result == .Success and test_result == .Success) {
    if (config.enable_council) {
        const rating = try council.convene(solution_code);
        report.addRating(rating);
    }
} else {
    report.addRating("N/A (Build Failed)");
}
```

---

## OpenRouter Integration

### Why OpenRouter?

- **Single API client** â€” All models use OpenAI-compatible JSON format
- **Wide model access** â€” Claude, GPT-4, Llama, Gemini, DeepSeek, etc.
- **Normalized usage** â€” Token counts in consistent format
- **Future flexibility** â€” Easy to add new models as they release

### API Response Format

```json
{
  "id": "gen-123...",
  "model": "anthropic/claude-3-opus",
  "choices": [ { "message": { "content": "..." } } ],
  "usage": {
    "prompt_tokens": 150,
    "completion_tokens": 340,
    "total_tokens": 490
  }
}
```

### Cost Calculation

Option 1 (Recommended): Hardcoded lookup table in `config.zig`
```zig
const COSTS = .{
    .{ "anthropic/claude-3.5-sonnet", 3.0, 15.0 },  // $/M input, $/M output
    .{ "openai/gpt-4o", 2.5, 10.0 },
    .{ "meta/llama-3-70b", 0.5, 0.5 },
};
```

Option 2: Call `/api/v1/generation?id=...` for exact cost (extra API call)

---

## Report Output

### ASCII Table (Default)

```
BENCHMARK REPORT
================================================================================
MODEL                   | TIME   | SCORE | COST    | MEM SAFETY | LOC | RATING
--------------------------------------------------------------------------------
anthropic/claude-3.5    | 4.2s   | 3/3   | $0.0042 | PASS       | 145 | S (9.5)
openai/gpt-4o           | 2.8s   | 3/3   | $0.0038 | PASS       | 120 | A (8.8)
meta/llama-3-70b        | 5.1s   | 1/3   | $0.0005 | FAIL       | 160 | C (6.0)
--------------------------------------------------------------------------------
* Rating provided by "The Council" (3-judge consensus)
```

### Computed Metrics

| Metric | Source |
|--------|--------|
| TIME | `std.time.nanoTimestamp()` around LLM request |
| SCORE | Count of passed `zig test` runs (0-3) |
| COST | `(input_tokens Ã— input_price) + (output_tokens Ã— output_price)` |
| MEM SAFETY | `GeneralPurposeAllocator` with `.detect_leaks = true` |
| LOC | Line count after stripping comments/whitespace |
| RATING | Council consensus score (S/A/B/C/F scale) |

---

## Configuration

### Environment Variables

```bash
OPENROUTER_API_KEY=sk-or-v1-...
```

### CLI Arguments

| Flag | Description | Default |
|------|-------------|---------|
| `--models` | Comma-separated model IDs | required |
| `--runs` | Runs per model per problem | 1 |
| `--council` | Enable Council judging | false |
| `--output` | Output format (ascii/json) | ascii |
| `--parallel` | Max concurrent requests | 4 |

---

## Development Roadmap

### Phase 1: Core Benchmark
- [ ] OpenRouter HTTP client
- [ ] Problem prompt loading
- [ ] Code extraction from responses
- [ ] Sandbox execution (zig test)
- [ ] Basic ASCII reporter

### Phase 2: Metrics & Cost
- [ ] Token usage parsing
- [ ] Cost calculation
- [ ] LOC counting
- [ ] Memory leak detection

### Phase 3: Council
- [ ] Multi-judge orchestration
- [ ] Phase 1/2 prompt construction
- [ ] Consensus scoring

### Phase 4: Polish
- [ ] JSON output format
- [ ] Multiple runs per problem
- [ ] Result caching
- [ ] Historical comparison
- [ ] test giving a semantic search indexed info for zig v0.15.2 so older models can have access to updated features and syntax in zig that may have missed their training (and stop the model from searching the web and wasting tokens)
