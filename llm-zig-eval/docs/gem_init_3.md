This approach is often called a **"Mixture of Judges"** or **"Consensus Evaluation"**. It significantly reduces the bias of a single model favoring its own coding style. Since we are using OpenRouter, we can easily cast three distinct "personalities" for the Council (e.g., GPT-4o, Claude 3.5 Sonnet, and DeepSeek V3) to act as the tribunal.

Here is the updated architecture for the **Council of Elders**.

### 1. ASCII Diagram: The Consensus Protocol

This flow happens *after* the code passes (or fails) the compiler.

```text
+-------------------------------------------------------+
|                 CANDIDATE SOLUTION                    |
|             (The Zig code generated by Model X)       |
+-------------------------------------------------------+
                           |
            +--------------+--------------+
            |              |              |
            v              v              v
   +----------------+ +----------------+ +----------------+
   |    JUDGE A     | |    JUDGE B     | |    JUDGE C     |
   | (Claude 3.5)   | |    (GPT-4o)    | | (DeepSeek V3)  |
   +----------------+ +----------------+ +----------------+
            |              |              |
            | (Phase 1: Independent Blind Grading)
            v              v              v
   [  Draft Score ]   [  Draft Score ]   [  Draft Score ]
            |              |              |
            +--------------+--------------+
                           |
             < CROSS-POLLINATION OF CRITIQUES >
   "Judge A, here is what B and C thought. Re-eval?"
                           |
            +--------------+--------------+
            |              |              |
            v              v              v
   +----------------+ +----------------+ +----------------+
   | FINAL VERDICT  | | FINAL VERDICT  | | FINAL VERDICT  |
   |     9/10       | |      8/10      | |     8.5/10     |
   +----------------+ +----------------+ +----------------+
                           |
                           v
               +-----------------------+
               |   CONSENSUS ENGINE    |
               | (Average: 8.5 / 10)   |
               +-----------------------+
```

---

### 2. Updated File Structure

We add a dedicated `council` module to handle the multi-threaded negotiation.

```text
swe-zig-bench/
├── ...
├── src/
│   ├── main.zig
│   ├── gateways/
│   │   └── openrouter.zig     # Generic API client
│   ├── council/
│   │   ├── types.zig          # Structs: Score, Critique, JudgeProfile
│   │   ├── tribunal.zig       # Orchestrates the 3 threads & message passing
│   │   └── prompts.zig        # The "Constitution" (System Prompts)
│   ├── core/
│   │   └── ...
└── ...
```

---

### 3. The Council Logic (Pseudo-Zig)

Here is how we implement the 3-step verification in `src/council/tribunal.zig`.

#### Phase 1: Blind Evaluation
We spawn 3 distinct threads. We map specific models to "Judge Personas" to ensure diversity.

*   **Judge 1 (The Pedant):** Claude 3.5 Sonnet (Focus: Safety, `defer`, strict types)
*   **Judge 2 (The Architect):** GPT-4o (Focus: Readability, structure, logic)
*   **Judge 3 (The Hacker):** DeepSeek/Llama-3 (Focus: Performance, cleverness, brevity)

**System Prompt (Phase 1):**
> "You are a Judge on the Zig Council. Analyze the user's code.
> Criteria: 1. Memory Safety (Leaks?), 2. Idiomatic Zig, 3. Performance.
> Output JSON: `{ "score": 0-10, "critique": "..." }`"

#### Phase 2: Anonymous Peer Review
Once all 3 JSONs return, we format a new prompt for **Phase 2**.

**System Prompt (Phase 2):**
> "You previously rated this code [X]/10.
> Here are the opinions of two other senior engineers:
> Peer 1 said: '[Insert Critique B]'
> Peer 2 said: '[Insert Critique C]'
>
> Reflect on their points. If you missed a memory leak they found, lower your score. If they praised an optimization you missed, raise it.
> Output Final JSON: `{ "final_score": 0-10, "consensus_notes": "..." }`"

---

### 4. Implementation Details: The "Constitution"

To make this work, we need a robust prompt structure in `src/council/prompts.zig`.

**The Grading Rubric (Injected into System Prompt):**
1.  **Safety (Critical):** Does it use the passed allocator? Does it `defer` correctly? Are there potential Use-After-Frees?
2.  **Correctness:** Does it actually solve the problem described?
3.  **Zig-Zen:** Does it use optional chaining (`.?`), `try`, and slices rather than C-style pointer arithmetic where possible?

**Example JSON Response Parsing (Zig):**
We use `std.json` to parse the LLM response.

```zig
// src/council/types.zig

pub const JudgeVerdict = struct {
    score: f32,
    critique: []const u8,
    // Reasoning for the score
    reasoning: []const u8, 
};

pub const ConsensusResult = struct {
    final_score: f32,
    judges: [3]JudgeVerdict,
};
```

---

### 5. Benchmark Scenario: How the Council Reacts
Let's look at how this setup evaluates one of our questions.

**The Question:** "Create a Thread-Safe Queue using `std.Thread.Mutex`."
**The Candidate Code (from Model X):** Uses a Mutex but forgets to `unlock()` in one specific error branch.

**Phase 1 (Blind):**
*   **Judge A (Claude):** "Score: 6/10. Critical: Mutex is locked but not unlocked if `alloc` fails on line 14. Potential deadlock."
*   **Judge B (GPT-4o):** "Score: 9/10. Great naming conventions. Very readable code."
*   **Judge C (DeepSeek):** "Score: 8/10. Good use of `std.ArrayList`. Logic seems sound."

**Phase 2 (The Swap):**
*   *System sends Judge A's finding to B and C.*
*   **Judge B (Reflection):** "I missed the deadlock on line 14 mentioned by Peer 1. That is a critical failure. Adjusting score to 5/10."
*   **Judge C (Reflection):** "Agreed. Deadlocks are unacceptable in Zig concurrency. Lowering to 5/10."

**Final Result:**
*   **Average Score:** 5.3/10 (The deadlock was caught via consensus).
*   *Without the council, GPT-4o would have given it a 9, skewing the benchmark.*

### 6. Cost Estimation for the Council
Since we are hitting the API 3 times (Phase 1) + 3 times (Phase 2) per problem, the cost is higher.
*   **Strategy:** Only run the Council if the code **PASSES** the compiler and tests. There is no need to pay for a philosophical debate on code that doesn't compile.

**Main Loop Logic (`src/main.zig`):**
```zig
if (compile_result == .Success and test_result == .Success) {
    if (config.enable_council) {
        const rating = try council.convene(solution_code);
        report.addRating(rating);
    }
} else {
    report.addRating("N/A (Build Failed)");
}
```